# Prototype to Performance
## Envisioning An Experience For Everyone
 type here.
 
## From Pixels To A Prototype
### Finding A Purpose for our Prototype
 type here.
 
### Design Taxonomy
  * **Crowd Participation**
    * Increased number of users enhances or creates an increased physical, emotional, media/computational response.  
       
### Area of Focus
  * **Media Architecture**
    * Computational display systems incorporated into the physical landscape including screens, lights, projection mapping and tracking.
    
### Sensory Environment:
  * **Visual Graphic**
    * 2D physical imaging technologies that use pigment or found objewcts, including priting, painting and mixed media.  
  * **Visual: Light**
    * Eminating or luminating a light source.
  * **Visual: Distortion**
    * Intentionally or randomly affecting the pattern of a series or sequence or light, pixels or images.  
  * **Visual: Projection**
    * Casting or throwing light from a source to a surface.  
  * **Sound: Distortion / Generation** 
    * Intentionally or randomly affecting the pattern of, or creating an audio wave or series.  
    
### BodyStorming
  * **Interactive Environments**
    * The idea is to have multiple controllers as "keys" in a walking exhibit or installation.  Whenever the user presses the button on their microcontroller it changes the environment around them, but not for everyone else.  
  * **Projection Mapping**
    * A projection mapped visual onto the ground.  When users walk onto the projection their position is tracked.  Pressing a button controls an aspect of the projection.  Could require multiple people.  
  * **Crowd Generated Music/Sound**
    * A collective crowd generated 
    
    ## Bodystorming
    
    Typing Words.
    
## Prototype 1.0

The visuals below are captured from a real time 3D rendering of our prototype made exclusively in TouchDesigner.  

![ImageOne](images/TDMovieOut.0.jpg)
![ImageTwo](images/TDMovieOut.1.jpg)
![ImageThree](images/TDMovieOut.2.jpg)
![ImageFour](images/TDMovieOut.3.jpg)

You can view a video preview of our visualized prototype 1.0 here : [Prototype One](https://www.youtube.com/watch?v=RlnMgWQJlpA&feature=youtu.be)

> The first prototype is a live demonstration of our concept, including the interaction between the Feather Huzzah and TouchDesigner.  By connecting to the Adafruit IO api, it is possible to stream the data directly from the api into touchdesigner as JSON format, and parse the information directly inside Touchdesigner.  Because of this It is possible for us to use the live input from the feather huzzah to control aspects of the touchdesigner visual.  In this prototype Michael's button changes the colour of the projection, while William's button effects the complexity and detail of the projected visual.  This 3D mockup allowed us to visualize and validate the claims of our process.  

## Options For Our Interactions
### Powering A Pixel Perfect Performance 
* **TouchDesigner**
    * The idea is to have multiple
* **Feather Huzzah**
    * The idea is to have multiple
* **[MQTT](https://mqtt.org/)**
    * The idea is to have multiple
* **[Socket.IO](https://socket.io/)**
    * The idea is to have multiple
* **[Dweet.io](https://mqtt.org/)**
    * The idea is to have multiple
* **[Data To mySQL Via ES8266](https://theiotprojects.com/insert-data-into-mysql-database-with-esp8266/) **
    * The idea is to have multiple

### Coming soon
## Prototype 1.1
[Prototype 1.1](https://www.youtube.com/watch?v=__j6FiRErwo&feature=youtu.be)
